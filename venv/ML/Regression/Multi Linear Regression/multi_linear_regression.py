# Section: importing librariesimport numpy as npimport matplotlib.pyplot as pltimport pandas as pd# Section: Importing data setsdataset = pd.read_csv('50_Startups.csv')# x is a matrix of dependent variablesX = dataset.iloc[:, :-1].valuesy = dataset.iloc[:, 4].values# Encoding categorical data and independent variablefrom sklearn.preprocessing import LabelEncoder, OneHotEncoderlabelencoder_X = LabelEncoder()X[:, 3] = labelencoder_X.fit_transform(X[:, 3])  # here 3 is the index of categorical columnohCoder = OneHotEncoder(categorical_features=[3])  # here 3 is the index of categorical columnX = ohCoder.fit_transform(X).toarray()# Taking care of dummy variable# Here we eliminated 1st dummy column as if other two columns are 0,0 then its california for sure.. right ?X = X[:, 1:]# Section: splitting data into training and test setfrom sklearn.model_selection import train_test_split  # model_selection earlier cross_validationx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"""# Section: feature scaling, taken care by libraryfrom sklearn.preprocessing import StandardScalersc_X = StandardScaler()x_train = sc_X.fit_transform(x_train)x_test = sc_X.transform(x_test)"""# fitting multiple linear regression to the training setfrom sklearn.linear_model import LinearRegressionregressor = LinearRegression()regressor.fit(x_train, y_train)# predicting the test set resultsy_pred = regressor.predict(x_test)# building the optimal model using backward elimination (BE)import statsmodels.api as sm# our equation is : y= b0 + b1x1 + b2x2 . . . . + bnxn# if we directly do BE , then b0 will be excluded, So we need to introduce x0 ( = 1)X = np.append(arr=np.ones((50, 1)).astype(int), values=X, axis=1)X_opt = X[:, [0, 1, 2, 3, 4, 5]]regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()print('first elimination \n', regressor_OLS.summary())  # O/P as below:# Col list: R&D Spend, Administration, Marketing Spend, State, Profit'''                            OLS Regression Results                            ==============================================================================Dep. Variable:                      y   R-squared:                       0.951Model:                            OLS   Adj. R-squared:                  0.945Method:                 Least Squares   F-statistic:                     169.9Date:                Sat, 27 Jul 2019   Prob (F-statistic):           1.34e-27Time:                        18:55:27   Log-Likelihood:                -525.38No. Observations:                  50   AIC:                             1063.Df Residuals:                      44   BIC:                             1074.Df Model:                           5                                         Covariance Type:            nonrobust                                         ==============================================================================                 coef    std err          t      P>|t|      [0.025      0.975]------------------------------------------------------------------------------const       5.013e+04   6884.820      7.281      0.000    3.62e+04     6.4e+04x1           198.7888   3371.007      0.059      0.953   -6595.030    6992.607x2           -41.8870   3256.039     -0.013      0.990   -6604.003    6520.229x3             0.8060      0.046     17.369      0.000       0.712       0.900x4            -0.0270      0.052     -0.517      0.608      -0.132       0.078x5             0.0270      0.017      1.574      0.123      -0.008       0.062==============================================================================Omnibus:                       14.782   Durbin-Watson:                   1.283Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.266Skew:                          -0.948   Prob(JB):                     2.41e-05Kurtosis:                       5.572   Cond. No.                     1.45e+06=============================================================================='''#  the highest P value is of x2=0.990 i.e. index:2 which is above significance value (0.05)#  so we remove this from our list of predictorsX_opt = X[:, [0, 1, 3, 4, 5]]regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()print('\n Second elimination \n ', regressor_OLS.summary())'''==============================================================================                 coef    std err          t      P>|t|      [0.025      0.975]------------------------------------------------------------------------------const       5.011e+04   6647.870      7.537      0.000    3.67e+04    6.35e+04x1           220.1585   2900.536      0.076      0.940   -5621.821    6062.138 => max and > 0.05, remove 2nd predictorx2             0.8060      0.046     17.606      0.000       0.714       0.898x3            -0.0270      0.052     -0.523      0.604      -0.131       0.077x4             0.0270      0.017      1.592      0.118      -0.007       0.061=============================================================================='''X_opt = X[:, [0, 3, 4, 5]]regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()print('\n third elimination \n ', regressor_OLS.summary())'''const       5.012e+04   6572.353      7.626      0.000    3.69e+04    6.34e+04x1             0.8057      0.045     17.846      0.000       0.715       0.897x2            -0.0268      0.051     -0.526      0.602      -0.130       0.076 => max and > 0.05, remove 2nd predictorx3             0.0272      0.016      1.655      0.105      -0.006       0.060'''X_opt = X[:, [0, 3, 5]]regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()print('\n fourth elimination \n ', regressor_OLS.summary())'''const       4.698e+04   2689.933     17.464      0.000    4.16e+04    5.24e+04x1             0.7966      0.041     19.266      0.000       0.713       0.880x2             0.0299      0.016      1.927      0.060      -0.001       0.061 => max and > 0.05, remove 2nd predictor'''X_opt = X[:, [0, 3]]regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()print('\n fifth elimination \n ', regressor_OLS.summary())'''const       4.903e+04   2537.897     19.320      0.000    4.39e+04    5.41e+04x1             0.8543      0.029     29.151      0.000       0.795       0.913 => nothing > 0.05, we finalize our equation here.  hence, 3rd element in dataset (starting from 0) is the only predictor '''x_train, x_test, y_train, y_test = train_test_split(X_opt, y, test_size=0.2, random_state=0)regressor.fit(x_train, y_train)y_ols_pred = regressor.predict(x_test)print("  test_values \t\t predicted_values \t\t\t ols_predicted_values")for x in range(0, 10):    print(x, y_test[x], '\t', y_pred[x], '\t', y_ols_pred[x])'''          test_values 	          predicted_values 	         ols_predicted_values0         103282.38 	        103015.2015979623 	        104667.27805997861         144259.4 	            132582.27760815746 	        134150.83410578472         146121.95 	        132447.73845174874 	        135207.800195169233         77798.83 	            71976.098512589 	        72170.544288556354         191050.39 	        178537.48221054114 	        179090.586025083525         105008.31 	        116161.24230163363 	        109824.773865863396         81229.06 	            67851.69209676355 	        65644.277737567777         97483.56 	            98791.7337468797 	        100481.432771386558         110352.25 	        113969.43533011655 	        111431.752024321059         166187.94 	        167921.06569549997 	        169438.148435391'''